{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb1d537e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import gradio as gr\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain_core.output_parsers import (JsonOutputParser,\n",
    "                                           PydanticOutputParser)\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from mod.O_prompt import (STORY_GENERATOR_SYSTEM_PROMPT,\n",
    "                          STORY_GENERATOR_USER_PROMPT, SYSTEM_PROMPT)\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "664a82a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TurtleSoupStory(BaseModel):\n",
    "    \"\"\"\n",
    "    海龜湯（情境猜謎）的故事模型\n",
    "    \"\"\"\n",
    "    title: str = Field(description=\"故事標題\", examples=\"海龜湯\")\n",
    "    difficulty: str = Field(description=\"海龜湯故事的難易度\")\n",
    "    custom: str = Field(description=\"故事情節的客製化要素\")\n",
    "    question: str = Field(description=\"故事的謎面，隱藏了故事的關鍵真相，僅看到部分的結果\")\n",
    "    answer: str = Field(description=\"故事的謎底，包含所有的完整真相和邏輯環節\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597ef072",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StoryGenerator:\n",
    "    def __init__(self, llm, system_prompt):\n",
    "        self.llm = llm\n",
    "        self.messages = [SystemMessage(system_prompt)]\n",
    "\n",
    "    def generate_story(self, difficulty, custom, pydantic_schema):\n",
    "        data = {\n",
    "            \"difficulty\": difficulty,\n",
    "            \"custom\": custom,\n",
    "            \"pydantic_schema\": pydantic_schema\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f9678d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gemini-3-flash-preview'\n",
    "api_key = os.environ.get(\"GOOGLE_API\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(\n",
    "        model=model_name,\n",
    "        google_api_key=api_key\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8028c081",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_parser = JsonOutputParser()\n",
    "pydantic_parser = PydanticOutputParser(\n",
    "    pydantic_object=TurtleSoupStory\n",
    ")\n",
    "pydantic_schema = pydantic_parser.get_format_instructions()\n",
    "\n",
    "difficulty = input(\"請輸入困難度（簡單/普通/困難/隨機）\")\n",
    "custom = input(\"請輸入客製化故事需求\")\n",
    "\n",
    "data = {\n",
    "    \"difficulty\": difficulty,\n",
    "    \"custom\": custom,\n",
    "    \"pydantic_schema\": pydantic_schema\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e614a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "story_gen_system = STORY_GENERATOR_SYSTEM_PROMPT\n",
    "story_gen_user = STORY_GENERATOR_USER_PROMPT\n",
    "\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "        (\"system\", story_gen_system),\n",
    "        (\"human\", story_gen_user)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "44430a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "story_gen_chain = prompt_template | llm | json_parser\n",
    "\n",
    "story = story_gen_chain.invoke(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb0ed39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bot:\n",
    "    def __init__(self, llm, system_prompt):\n",
    "        self.llm = llm\n",
    "        self.messages = [SystemMessage(system_prompt)]\n",
    "\n",
    "    def chat_stream(self, text):\n",
    "        self.messages.append(HumanMessage(text))\n",
    "\n",
    "        full_response = []\n",
    "\n",
    "        for chunk in self.llm.stream(self.messages):\n",
    "            if chunk.content:\n",
    "                if isinstance(chunk.content, list):\n",
    "                    for block in chunk.content:\n",
    "                        full_response.append(block)\n",
    "\n",
    "                        if block.get('type') == 'text':\n",
    "                            yield block.get('text', '')\n",
    "                else:\n",
    "                    full_response.append({\"type\": \"text\", \"text\": chunk.content})\n",
    "                    yield chunk.content\n",
    "\n",
    "        self.messages.append(AIMessage(full_response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "19d0778b",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = SYSTEM_PROMPT.format(**story)\n",
    "\n",
    "bot = Bot(llm=llm, system_prompt=system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0b2fc6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def chat_function_stream(message, history):\n",
    "    \"\"\"\n",
    "    處理每輪使用者輸入的訊息，並以串流方式回傳回應。\n",
    "\n",
    "    參數：\n",
    "        message (str)：使用者輸入的文字\n",
    "        history (list)：對話歷史紀錄，每輪對話包含問答內容\n",
    "\n",
    "    回傳：\n",
    "        生成器 (generator)：逐步產生回覆的文字片段，可即時在 Gradio 顯示\n",
    "    \"\"\"\n",
    "    full_response = \"\"\n",
    "\n",
    "    for chunk in bot.chat_stream(message):\n",
    "        # 將每次生成的片段累加\n",
    "        full_response += chunk\n",
    "        # 使用 yield 將目前累積的回覆回傳給 Gradio\n",
    "        yield full_response\n",
    "\n",
    "webui = gr.ChatInterface(chat_function_stream)\n",
    "webui.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "situation-puzzles-with-llm-yY4XmfWK-py3.13",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
